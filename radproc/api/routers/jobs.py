# radproc/api/routers/jobs.py
# radproc/api/routers/jobs.py
import io
import os
import logging
import uuid
import pandas as pd
from fastapi import APIRouter, HTTPException, status, Body, Depends, Query, Response
from fastapi.responses import StreamingResponse, JSONResponse, FileResponse
from datetime import datetime, timezone
from huey.exceptions import TaskException
from mimetypes import guess_type
from typing import Optional, Literal, Any, Tuple

from ...huey_config import huey
from ...tasks import run_generate_point_timeseries, run_calculate_accumulation, run_create_animation
from ..schemas.jobs import (
    TimeseriesJobRequest,
    AccumulationJobRequest,
    AnimationJobRequest,
    JobSubmissionResponse,
    JobStatusResponse,
    JobStatus,
)
from ..dependencies import get_core_config, get_api_job_output_dir
from ...core.config import get_point_config, get_setting # Keep get_setting
# --- NEW DB IMPORTS ---
from ...core.db_manager import (
    get_connection, release_connection, get_or_create_variable_id,
    query_timeseries_data_for_point, get_point_config_from_db
)
# --------------------

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/jobs",
    tags=["Background Jobs"],
)

# --- POST /jobs/timeseries ---
@router.post(
    "/timeseries",
    response_model=JobSubmissionResponse,
    status_code=status.HTTP_202_ACCEPTED,
    summary="Queue a job to generate/update historical timeseries data",
)
async def queue_timeseries_job(
    request_body: TimeseriesJobRequest = Body(...),
    _config: dict = Depends(get_core_config),
):
    point_name = request_body.point_name
    start_dt = request_body.start_dt
    end_dt = request_body.end_dt
    variable = request_body.variable
    source = request_body.source
    version = request_body.version

    logger.info(
        f"Received request to queue timeseries job for point: '{point_name}', "
        f"Range: {start_dt.isoformat()} to {end_dt.isoformat()}, Var: {variable or 'Default'}"
    )

    # Use get_point_config_from_db for validation now
    try:
        conn_temp = get_connection()
        if not get_point_config_from_db(conn_temp, point_name):
            logger.warning(f"Timeseries job request failed: Point '{point_name}' not found in DB.")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Point '{point_name}' not found.",
            )

        if conn_temp: release_connection(conn_temp)
    except Exception as e:
         logger.error(f"DB Error during point validation: {e}")
         raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Database error during validation.")


    if start_dt >= end_dt:
        logger.warning(f"Timeseries job request failed: Start datetime must be before end datetime.")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Start datetime must be before end datetime.",
        )

    try:
        task = run_generate_point_timeseries(
            point_name, # Pass point_name directly
            start_dt.isoformat(),
            end_dt.isoformat(),
            source,
            version,
            variable,
        )
        task_id = task.id
        logger.info(f"Timeseries job queued successfully for point '{point_name}'. Task ID: {task_id}")
        return JobSubmissionResponse(
            message="Timeseries generation job queued.",
            job_type="timeseries",
            task_id=task_id,
        )
    except Exception as e:
        logger.exception(f"Failed to queue timeseries job for point '{point_name}': {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to queue job: An internal error occurred.",
        )

# POST /accumulation and /animation remain the same...

# GET /status remains the same...

# --- GET /jobs/timeseries/{task_id}/data (MODIFIED) ---
@router.get(
    "/timeseries/{task_id}/data",
    summary="Retrieve the data generated by a completed timeseries job from DB",
    responses={
        200: {"description": "Data retrieved successfully (JSON or CSV)."},
        202: {"description": "Job is still pending or running."},
        400: {"description": "Job failed or was revoked."},
        404: {"description": "Job ID not found or required data missing."},
        500: {"description": "Internal server error or DB error."},
    }
)
async def get_timeseries_job_data(
    task_id: str,
    format: Literal["json", "csv"] = Query("json", description="Output data format"),
    # Add an optional variable query param
    variable: Optional[str] = Query(None, description="Specify which variable's data to retrieve (if job processed defaults)."),
    # Keep time filters, as they query the DB result
    start_dt: Optional[datetime] = Query(None, description="Filter data >= this datetime (UTC ISO format recommended)"),
    end_dt: Optional[datetime] = Query(None, description="Filter data <= this datetime (UTC ISO format recommended)"),
    interval: Optional[str] = Query(None, description="Resample data to this Pandas interval (e.g., '1H', '15min')"),
):
    logger.info(f"Request for DB timeseries data from job ID: {task_id}, Variable: {variable or 'Auto/First'}")
    job_status, job_result_payload, error_info = await _get_job_outcome(task_id)

    if job_status == "PENDING":
        return Response(status_code=status.HTTP_202_ACCEPTED, content=f"Job {task_id} is pending.")
    elif job_status == "FAILURE":
        raise HTTPException(status.HTTP_400_BAD_REQUEST, detail=f"Job {task_id} failed: {error_info}")
    elif job_status == "REVOKED":
        raise HTTPException(status.HTTP_400_BAD_REQUEST, detail=f"Job {task_id} was revoked.")
    elif job_status == "SUCCESS":
        conn = None
        try:
            if not isinstance(job_result_payload, dict):
                 raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Job completed with unexpected result format.")

            point_name = job_result_payload.get("point_name")
            job_start_iso = job_result_payload.get("start_dt_iso")
            job_end_iso = job_result_payload.get("end_dt_iso")
            variable_processed = job_result_payload.get("variable_processed")
            source_version = job_result_payload.get("source_version")

            if not all([point_name, job_start_iso, job_end_iso, variable_processed]):
                raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Job result missing necessary parameters.")

            # Determine which variable to query
            variable_to_query = variable # Use query param if provided
            if not variable_to_query:
                if variable_processed != "defaults":
                    variable_to_query = variable_processed # Use the specific var from job
                else:
                    # If job ran defaults, and no specific var requested, pick the first default
                    default_vars = get_setting('app.timeseries_default_variables', [])
                    if not default_vars:
                         raise HTTPException(status.HTTP_404_NOT_FOUND, "No specific variable requested and no default variables configured.")
                    variable_to_query = default_vars[0]
                    logger.warning(f"No specific variable requested for job {task_id}, using first default: {variable_to_query}")

            # --- Query DB ---
            conn = get_connection()
            point_config = get_point_config_from_db(conn, point_name)
            if not point_config or not point_config.get('point_id'):
                 raise HTTPException(status.HTTP_404_NOT_FOUND, f"Point '{point_name}' not found in DB.")
            point_id = point_config['point_id']

            variable_id = get_or_create_variable_id(conn, variable_to_query)
            if variable_id is None:
                 raise HTTPException(status.HTTP_404_NOT_FOUND, f"Variable '{variable_to_query}' not found in DB.")

            # Use job's range unless overridden by query params
            query_start_dt = start_dt if start_dt else datetime.fromisoformat(job_start_iso.replace('Z', '+00:00'))
            query_end_dt = end_dt if end_dt else datetime.fromisoformat(job_end_iso.replace('Z', '+00:00'))

            # Ensure TZ awareness
            if query_start_dt.tzinfo is None: query_start_dt = query_start_dt.replace(tzinfo=timezone.utc)
            if query_end_dt.tzinfo is None: query_end_dt = query_end_dt.replace(tzinfo=timezone.utc)

            df = query_timeseries_data_for_point(conn, point_id, variable_id, query_start_dt, query_end_dt, variable_to_query, source_version,)
            # --- End Query DB ---

            # Resample if requested
            if interval and not df.empty:
                try:
                    df_indexed = df.set_index("timestamp")
                    df_resampled = df_indexed.resample(interval).mean() # Using mean, adjust if needed
                    df = df_resampled.reset_index()
                except ValueError:
                     raise HTTPException(status.HTTP_400_BAD_REQUEST, f"Invalid resampling interval: {interval}")
                except Exception as resample_err:
                     logger.error(f"Error resampling data for job {task_id}: {resample_err}", exc_info=True)
                     raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Failed to resample data.")

            # Format and return
            if format == "csv":
                 stream = io.StringIO()
                 df_out = df.copy()
                 if "timestamp" in df_out.columns and pd.api.types.is_datetime64_any_dtype(df_out["timestamp"]):
                      df_out["timestamp"] = df_out["timestamp"].dt.tz_convert('UTC').strftime("%Y-%m-%dT%H:%M:%SZ")
                 df_out.to_csv(stream, index=False)
                 content = stream.getvalue()
                 stream.close()
                 return Response(content=content, media_type="text/csv", headers={"Content-Disposition": f'attachment; filename="job_{task_id}_{variable_to_query}_data.csv"'})
            else: # JSON
                 if df.empty: return JSONResponse(content=[])
                 df_out = df.copy()
                 if "timestamp" in df_out.columns and pd.api.types.is_datetime64_any_dtype(df_out["timestamp"]):
                      df_out["timestamp"] = df_out["timestamp"].dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                 df_out = df_out.astype(object).where(pd.notnull(df_out), None)
                 json_data = df_out.to_dict(orient="records")
                 return JSONResponse(content=json_data)

        except HTTPException: raise
        except Exception as e:
             logger.exception(f"Failed to retrieve/process DB data for job {task_id}: {e}")
             raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Error processing/retrieving DB data.")
        finally:
             if conn: release_connection(conn)
    else:
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, f"Unexpected job status '{job_status}' encountered.")


# GET /accumulation/{task_id}/data remains the same (it relies on path)
# GET /animation/{task_id}/data remains the same (it relies on path)
# _get_job_outcome remains the same.
# Need to add GET /accumulation and /animation endpoints...

@router.post(
    "/accumulation",
    response_model=JobSubmissionResponse,
    status_code=status.HTTP_202_ACCEPTED,
    summary="Queue a job to calculate precipitation accumulation",
)
async def queue_accumulation_job(
    request_body: AccumulationJobRequest = Body(...),
    job_output_dir: str = Depends(get_api_job_output_dir)
):
    """
    Submits a background job to calculate accumulated precipitation for a point.
    """
    point_name = request_body.point_name
    start_dt = request_body.start_dt
    end_dt = request_body.end_dt
    interval = request_body.interval
    rate_variable = request_body.rate_variable or "RATE" # Use default if None
    source = request_body.source
    version = request_body.version


    logger.info(f"Received request to queue accumulation job for point: '{point_name}', Interval: {interval}")

    # --- Validation ---
    try:
        conn_temp = get_connection()
        if not get_point_config_from_db(conn_temp, point_name):
            raise HTTPException(status.HTTP_404_NOT_FOUND, detail=f"Point '{point_name}' not found.")
        if conn_temp: release_connection(conn_temp)
    except Exception as e:
         logger.error(f"DB Error during point validation: {e}")
         raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Database error during validation.")

    if start_dt >= end_dt:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="Start datetime must be before end datetime.")

    try:
        safe_interval = interval.replace(':','-').replace('/','_')
        unique_id = uuid.uuid4()
        filename = f"{point_name}_{rate_variable}_{safe_interval}_acc_{unique_id}.csv"
        server_output_path = os.path.join(job_output_dir, filename)
        logger.info(f"Generated output path for accumulation job: {server_output_path}")
    except Exception as e:
        logger.error(f"Failed to generate output path for accumulation job: {e}", exc_info=True)
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to generate output path.")

    try:
        task = run_calculate_accumulation(
            point_name,
            start_dt.isoformat(),
            end_dt.isoformat(),
            interval,
            rate_variable,
            server_output_path,
            source,
            version
        )
        task_id = task.id
        logger.info(f"Accumulation job queued successfully for point '{point_name}'. Task ID: {task_id}")
        return JobSubmissionResponse(
            message="Accumulation calculation job queued.",
            job_type="accumulation",
            task_id=task_id,
        )
    except Exception as e:
        logger.exception(f"Failed to queue accumulation job for point '{point_name}': {e}")
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to queue job.")

@router.post(
    "/animation",
    response_model=JobSubmissionResponse,
    status_code=status.HTTP_202_ACCEPTED,
    summary="Queue a job to generate an animation",
)
async def queue_animation_job(
    request_body: AnimationJobRequest = Body(...),
    job_output_dir: str = Depends(get_api_job_output_dir)
):
    """
    Submits a background job to generate an animation from plot images.
    """
    variable=request_body.variable
    elevation=request_body.elevation
    start_dt=request_body.start_dt
    end_dt=request_body.end_dt
    output_format=request_body.output_format
    extent=request_body.extent
    no_watermark=request_body.no_watermark or False
    fps=request_body.fps

    logger.info(f"Received request to queue animation job: Var='{variable}', Elev={elevation}, Output_format='{output_format}'")

    if start_dt >= end_dt:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="Start datetime must be before end datetime.")

    try:
        start_str = start_dt.strftime('%Y%m%d%H%M')
        end_str = end_dt.strftime('%Y%m%d%H%M')
        unique_id = uuid.uuid4()
        filename = f"{variable}_{elevation}_{start_str}_{end_str}_anim_{unique_id}{output_format}"
        server_output_path = os.path.join(job_output_dir, filename)
        logger.info(f"Generated output path for animation job: {server_output_path}")
    except Exception as e:
        logger.error(f"Failed to generate output path for animation job: {e}", exc_info=True)
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to generate output path.")

    try:
        extent_list = list(extent) if extent else None

        task = run_create_animation(
            variable,
            elevation,
            start_dt.isoformat(),
            end_dt.isoformat(),
            server_output_path,
            extent_list,
            not no_watermark,
            fps
        )
        task_id = task.id
        logger.info(f"Animation job queued successfully for var '{variable}'. Task ID: {task_id}")
        return JobSubmissionResponse(
            message="Animation generation job queued.",
            job_type="animation",
            task_id=task_id,
        )
    except Exception as e:
        logger.exception(f"Failed to queue animation job for var '{variable}': {e}")
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to queue job.")

@router.get(
    "/{task_id}/status",
    response_model=JobStatusResponse,
    summary="Get the status of a background job",
    responses={
        200: {"description": "Status retrieved successfully."},
        404: {"description": "Job ID not found."},
        500: {"description": "Error checking job status."},
    }
)
async def get_job_status(task_id: str):
    """
    Retrieves the current status for a given job Task ID provided by Huey.

    Possible statuses:
    - **SUCCESS**: Task completed successfully. Result is available.
    - **FAILURE**: Task failed during execution. Error info is available.
    - **REVOKED**: Task was cancelled before execution could complete.
    - **PENDING**: Task is waiting in the queue, currently running, OR the Task ID is unknown/expired (Huey does not definitively distinguish these via this API).

    Note: If a Task ID is completely invalid or has expired from the result store,
    this endpoint will likely return a PENDING status. A 404 may occur only if
    the status check itself encounters certain errors.
    """
    logger.debug(f"Request received for status of job ID: {task_id}")

    task_status_str = "UNKNOWN"
    result_obj: Any = None
    error_info: Optional[str] = None
    last_update: Optional[datetime] = None

    try:
        # --- Attempt to retrieve the result or status ---
        # huey.result() might return:
        # - The actual result value if SUCCESSFUL and finished.
        # - An Exception object if FAILED and finished.
        # - None if PENDING, RUNNING, or potentially UNKNOWN ID.
        retrieved_value = None
        task_failed_exception = None

        try:
            retrieved_value = huey.result(task_id, preserve=True)
            logger.debug(f"huey.result() for {task_id} returned type: {type(retrieved_value)}")

        except TaskException as te:
            task_status_str = "FAILURE"
            task_failed_exception = te
            logger.warning(f"Job ID '{task_id}' indicated failure via TaskException.")
        except Exception as e:
             logger.exception(f"Unexpected error calling huey.result() for job ID '{task_id}': {e}")
             raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Error communicating with task queue backend.")

        if task_status_str == "FAILURE":
             original_error = getattr(task_failed_exception, 'metadata', task_failed_exception)
             error_info = f"{type(original_error).__name__}: {str(original_error)}"
             logger.warning(f"Job ID '{task_id}' failure detail: {error_info}")
             result_obj = None

        elif retrieved_value is None:
             if huey.is_revoked(task_id):
                 task_status_str = "REVOKED"
                 logger.info(f"Job ID '{task_id}' is revoked.")
             else:
                 task_status_str = "PENDING"
                 logger.debug(f"Job ID '{task_id}' is pending, running, or unknown (result not ready).")

        elif isinstance(retrieved_value, Exception):
             task_status_str = "FAILURE"
             original_error = retrieved_value
             error_info = f"{type(original_error).__name__}: {str(original_error)}"
             logger.warning(f"Job ID '{task_id}' reported failure (retrieved direct Exception): {error_info}")
             result_obj = None
        else:
             task_status_str = "SUCCESS"
             result_obj = retrieved_value
             if not isinstance(result_obj, (str, int, float, bool, list, dict, type(None))):
                 result_obj = str(result_obj) # Sanitize
             logger.debug(f"Job ID '{task_id}' finished successfully.")

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"General error retrieving status for job ID '{task_id}': {e}")
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Failed to retrieve job status.")

    if task_status_str == "UNKNOWN":
         logger.error(f"Could not determine final status for Task ID '{task_id}', treating as Not Found.")
         raise HTTPException(status.HTTP_404_NOT_FOUND, f"Could not determine status for Job ID '{task_id}'.")

    status_details = JobStatus(
        status=task_status_str,
        last_update=last_update,
        result=result_obj if task_status_str == "SUCCESS" else None,
        error_info=error_info if task_status_str == "FAILURE" else None,
    )

    job_type_placeholder = "unknown" # TODO: Implement job type storage/retrieval

    return JobStatusResponse(
        task_id=task_id,
        job_type=job_type_placeholder,
        status_details=status_details
    )

@router.get(
    "/accumulation/{task_id}/data",
    summary="Retrieve the CSV data from a completed accumulation job",
    response_class=Response,
    responses={
        200: {"content": {"text/csv": {}}, "description": "CSV data retrieved successfully."},
        202: {"description": "Job is still pending or running."},
        400: {"description": "Job failed or was revoked."},
        404: {"description": "Job ID not found or generated data file missing."},
        500: {"description": "Internal server error."},
    },
)
async def get_accumulation_job_data(task_id: str):
    logger.info(f"Request for data from accumulation job ID: {task_id}")
    job_status, job_result_payload, error_info = await _get_job_outcome(task_id)

    if job_status == "PENDING":
        return Response(status_code=status.HTTP_202_ACCEPTED, content=f"Job {task_id} is pending.")
    elif job_status == "FAILURE":
        raise HTTPException(status.HTTP_400_BAD_REQUEST, detail=f"Job {task_id} failed: {error_info}")
    elif job_status == "REVOKED":
         raise HTTPException(status.HTTP_400_BAD_REQUEST, detail=f"Job {task_id} was revoked.")
    elif job_status == "SUCCESS":
        try:
            if not isinstance(job_result_payload, dict):
                 raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Job completed with unexpected result format.")
            output_path = job_result_payload.get("output_path")
            if not output_path:
                 raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Job completed but output path is missing.")
            if not os.path.isfile(output_path):
                 raise HTTPException(status.HTTP_404_NOT_FOUND, f"Generated accumulation file not found for job {task_id}.")

            filename = os.path.basename(output_path)
            return FileResponse(path=output_path, media_type='text/csv', filename=filename)
        except HTTPException: raise
        except Exception as e:
            logger.exception(f"Error serving accumulation file for job {task_id}: {e}")
            raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Error serving generated file.")
    else:
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, f"Unexpected job status '{job_status}'.")

@router.get(
    "/animation/{task_id}/data",
    summary="Retrieve the animation file from a completed animation job",
    response_class=Response,
    responses={
        200: {"description": "Animation file retrieved successfully."},
        202: {"description": "Job is still pending or running."},
        400: {"description": "Job failed or was revoked."},
        404: {"description": "Job ID not found or generated data file missing."},
        500: {"description": "Internal server error."},
    },
)
async def get_animation_job_data(task_id: str):
    logger.info(f"Request for data from animation job ID: {task_id}")
    job_status, job_result_payload, error_info = await _get_job_outcome(task_id)

    if job_status == "PENDING":
        return Response(status_code=status.HTTP_202_ACCEPTED, content=f"Job {task_id} is pending.")
    elif job_status == "FAILURE":
        raise HTTPException(status.HTTP_400_BAD_REQUEST, detail=f"Job {task_id} failed: {error_info}")
    elif job_status == "REVOKED":
         raise HTTPException(status.HTTP_400_BAD_REQUEST, detail=f"Job {task_id} was revoked.")
    elif job_status == "SUCCESS":
        try:
            if not isinstance(job_result_payload, dict):
                 raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Job completed with unexpected result format.")
            output_path = job_result_payload.get("output_path")
            if not output_path:
                 raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Job completed but output path is missing.")
            if not os.path.isfile(output_path):
                 raise HTTPException(status.HTTP_404_NOT_FOUND, f"Generated animation file not found for job {task_id}.")

            filename = os.path.basename(output_path)
            media_type, _ = guess_type(filename)
            return FileResponse(
                path=output_path,
                media_type=media_type or 'application/octet-stream',
                filename=filename
            )
        except HTTPException: raise
        except Exception as e:
            logger.exception(f"Error serving animation file for job {task_id}: {e}")
            raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Error serving generated file.")
    else:
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Unexpected job status.")

async def _get_job_outcome(task_id: str) -> Tuple[str, Any, Optional[str]]:
    logger.debug(f"Helper _get_job_outcome checking task: {task_id}")
    try:
        retrieved_value = None
        task_failed_exception = None

        try:
            retrieved_value = huey.result(task_id, preserve=True)
            logger.debug(f"_get_job_outcome: huey.result() for {task_id} returned type: {type(retrieved_value)}")
        except TaskException as te:
            task_failed_exception = te
            logger.warning(f"_get_job_outcome: Job ID '{task_id}' indicated failure via TaskException.")
        except Exception as e:
             logger.exception(f"_get_job_outcome: Unexpected error calling huey.result() for job ID '{task_id}': {e}")
             raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, "Error communicating with task queue backend.")

        if task_failed_exception:
            original_error = getattr(task_failed_exception, 'metadata', task_failed_exception)
            error_info = f"{type(original_error).__name__}: {str(original_error)}"
            return "FAILURE", None, error_info
        elif retrieved_value is None:
            if huey.is_revoked(task_id):
                return "REVOKED", None, None
            else:
                return "PENDING", None, None
        elif isinstance(retrieved_value, Exception):
             original_error = retrieved_value
             error_info = f"{type(original_error).__name__}: {str(original_error)}"
             return "FAILURE", None, error_info
        else:
             return "SUCCESS", retrieved_value, None

    except Exception as e:
        logger.exception(f"_get_job_outcome: General error processing status for job ID '{task_id}': {e}")
        raise HTTPException(status.HTTP_404_NOT_FOUND, detail=f"Failed to check status or Job ID '{task_id}' not found.")